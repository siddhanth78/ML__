{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wAiLTwbWbBgL"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from tensorflow.keras.datasets import cifar10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d3a-0clekN3",
        "outputId": "c9ec5463-6243-48ac-c82d-3915334bfdb5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcSaEHKfbcsF",
        "outputId": "2792838b-5c1e-4a9a-924c-f60b4ba3c257"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 32, 32, 3)\n",
            "(50000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0"
      ],
      "metadata": {
        "id": "baB859vhbls-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_model():\n",
        "    inputs = keras.Input((32, 32, 3), name='input')\n",
        "    x = layers.Conv2D(32, 3,\n",
        "                      name='conv2d',\n",
        "                     padding='same',\n",
        "                     kernel_regularizer=regularizers.l2(0.01))(inputs)\n",
        "    x = layers.BatchNormalization(name='batch_norm1')(x)\n",
        "    x = keras.activations.relu(x)\n",
        "    x = layers.MaxPooling2D(pool_size=(2,2), name='maxpool')(x) #(2,2) is default\n",
        "    x = layers.Conv2D(64, 3,\n",
        "                      name='conv2d_2',\n",
        "                     padding='same',\n",
        "                     kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "    x = layers.BatchNormalization(name='batch_norm2')(x)\n",
        "    x = keras.activations.relu(x)\n",
        "    x = layers.MaxPooling2D(name='maxpool_2')(x)\n",
        "    x = layers.Conv2D(128, 3,\n",
        "                      name='conv2d_3',\n",
        "                     padding='same',\n",
        "                     kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "    x = layers.BatchNormalization(name='batch_norm3')(x)\n",
        "    x = keras.activations.relu(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(64,\n",
        "                     activation='relu',\n",
        "                     name='dense',\n",
        "                    kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "    x = layers.Dropout(0.5, name='dropout')(x)\n",
        "    outputs = layers.Dense(10, name='output')(x)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "model = my_model()\n",
        "\n",
        "model.compile(\n",
        "\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=3e-4),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXSY4-kDboP_",
        "outputId": "d0849c44-7994-498b-a85d-31fb2b414711"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input (InputLayer)          [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " batch_norm1 (BatchNormaliz  (None, 32, 32, 32)        128       \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " tf.nn.relu_3 (TFOpLambda)   (None, 32, 32, 32)        0         \n",
            "                                                                 \n",
            " maxpool (MaxPooling2D)      (None, 16, 16, 32)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " batch_norm2 (BatchNormaliz  (None, 16, 16, 64)        256       \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " tf.nn.relu_4 (TFOpLambda)   (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " maxpool_2 (MaxPooling2D)    (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " batch_norm3 (BatchNormaliz  (None, 8, 8, 128)         512       \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " tf.nn.relu_5 (TFOpLambda)   (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 8192)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                524352    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " output (Dense)              (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 619146 (2.36 MB)\n",
            "Trainable params: 618698 (2.36 MB)\n",
            "Non-trainable params: 448 (1.75 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, batch_size=64, epochs=150, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0O68kujbsVf",
        "outputId": "ffc60d25-93b4-4800-b643-211156b6ed3a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "782/782 - 6s - loss: 3.0258 - accuracy: 0.2928 - 6s/epoch - 8ms/step\n",
            "Epoch 2/150\n",
            "782/782 - 4s - loss: 1.9204 - accuracy: 0.4142 - 4s/epoch - 5ms/step\n",
            "Epoch 3/150\n",
            "782/782 - 4s - loss: 1.6816 - accuracy: 0.4640 - 4s/epoch - 5ms/step\n",
            "Epoch 4/150\n",
            "782/782 - 4s - loss: 1.5644 - accuracy: 0.4965 - 4s/epoch - 5ms/step\n",
            "Epoch 5/150\n",
            "782/782 - 4s - loss: 1.5009 - accuracy: 0.5147 - 4s/epoch - 5ms/step\n",
            "Epoch 6/150\n",
            "782/782 - 4s - loss: 1.4617 - accuracy: 0.5296 - 4s/epoch - 5ms/step\n",
            "Epoch 7/150\n",
            "782/782 - 4s - loss: 1.4274 - accuracy: 0.5385 - 4s/epoch - 5ms/step\n",
            "Epoch 8/150\n",
            "782/782 - 4s - loss: 1.4076 - accuracy: 0.5461 - 4s/epoch - 5ms/step\n",
            "Epoch 9/150\n",
            "782/782 - 4s - loss: 1.3791 - accuracy: 0.5550 - 4s/epoch - 5ms/step\n",
            "Epoch 10/150\n",
            "782/782 - 4s - loss: 1.3686 - accuracy: 0.5617 - 4s/epoch - 5ms/step\n",
            "Epoch 11/150\n",
            "782/782 - 4s - loss: 1.3517 - accuracy: 0.5700 - 4s/epoch - 5ms/step\n",
            "Epoch 12/150\n",
            "782/782 - 4s - loss: 1.3390 - accuracy: 0.5754 - 4s/epoch - 5ms/step\n",
            "Epoch 13/150\n",
            "782/782 - 4s - loss: 1.3246 - accuracy: 0.5792 - 4s/epoch - 5ms/step\n",
            "Epoch 14/150\n",
            "782/782 - 4s - loss: 1.3163 - accuracy: 0.5863 - 4s/epoch - 6ms/step\n",
            "Epoch 15/150\n",
            "782/782 - 4s - loss: 1.3009 - accuracy: 0.5890 - 4s/epoch - 5ms/step\n",
            "Epoch 16/150\n",
            "782/782 - 4s - loss: 1.2890 - accuracy: 0.5970 - 4s/epoch - 5ms/step\n",
            "Epoch 17/150\n",
            "782/782 - 4s - loss: 1.2782 - accuracy: 0.6037 - 4s/epoch - 6ms/step\n",
            "Epoch 18/150\n",
            "782/782 - 4s - loss: 1.2727 - accuracy: 0.6037 - 4s/epoch - 5ms/step\n",
            "Epoch 19/150\n",
            "782/782 - 4s - loss: 1.2611 - accuracy: 0.6086 - 4s/epoch - 5ms/step\n",
            "Epoch 20/150\n",
            "782/782 - 4s - loss: 1.2559 - accuracy: 0.6098 - 4s/epoch - 5ms/step\n",
            "Epoch 21/150\n",
            "782/782 - 4s - loss: 1.2424 - accuracy: 0.6186 - 4s/epoch - 5ms/step\n",
            "Epoch 22/150\n",
            "782/782 - 4s - loss: 1.2407 - accuracy: 0.6172 - 4s/epoch - 5ms/step\n",
            "Epoch 23/150\n",
            "782/782 - 4s - loss: 1.2337 - accuracy: 0.6213 - 4s/epoch - 5ms/step\n",
            "Epoch 24/150\n",
            "782/782 - 4s - loss: 1.2253 - accuracy: 0.6266 - 4s/epoch - 5ms/step\n",
            "Epoch 25/150\n",
            "782/782 - 4s - loss: 1.2248 - accuracy: 0.6267 - 4s/epoch - 5ms/step\n",
            "Epoch 26/150\n",
            "782/782 - 4s - loss: 1.2082 - accuracy: 0.6348 - 4s/epoch - 5ms/step\n",
            "Epoch 27/150\n",
            "782/782 - 4s - loss: 1.2048 - accuracy: 0.6344 - 4s/epoch - 6ms/step\n",
            "Epoch 28/150\n",
            "782/782 - 4s - loss: 1.1989 - accuracy: 0.6373 - 4s/epoch - 5ms/step\n",
            "Epoch 29/150\n",
            "782/782 - 4s - loss: 1.1962 - accuracy: 0.6388 - 4s/epoch - 5ms/step\n",
            "Epoch 30/150\n",
            "782/782 - 4s - loss: 1.1962 - accuracy: 0.6392 - 4s/epoch - 5ms/step\n",
            "Epoch 31/150\n",
            "782/782 - 4s - loss: 1.1851 - accuracy: 0.6428 - 4s/epoch - 5ms/step\n",
            "Epoch 32/150\n",
            "782/782 - 4s - loss: 1.1862 - accuracy: 0.6442 - 4s/epoch - 5ms/step\n",
            "Epoch 33/150\n",
            "782/782 - 4s - loss: 1.1797 - accuracy: 0.6483 - 4s/epoch - 5ms/step\n",
            "Epoch 34/150\n",
            "782/782 - 4s - loss: 1.1706 - accuracy: 0.6509 - 4s/epoch - 5ms/step\n",
            "Epoch 35/150\n",
            "782/782 - 4s - loss: 1.1774 - accuracy: 0.6477 - 4s/epoch - 5ms/step\n",
            "Epoch 36/150\n",
            "782/782 - 4s - loss: 1.1657 - accuracy: 0.6548 - 4s/epoch - 5ms/step\n",
            "Epoch 37/150\n",
            "782/782 - 4s - loss: 1.1686 - accuracy: 0.6522 - 4s/epoch - 5ms/step\n",
            "Epoch 38/150\n",
            "782/782 - 4s - loss: 1.1571 - accuracy: 0.6585 - 4s/epoch - 5ms/step\n",
            "Epoch 39/150\n",
            "782/782 - 4s - loss: 1.1578 - accuracy: 0.6592 - 4s/epoch - 5ms/step\n",
            "Epoch 40/150\n",
            "782/782 - 5s - loss: 1.1544 - accuracy: 0.6604 - 5s/epoch - 6ms/step\n",
            "Epoch 41/150\n",
            "782/782 - 4s - loss: 1.1446 - accuracy: 0.6649 - 4s/epoch - 5ms/step\n",
            "Epoch 42/150\n",
            "782/782 - 4s - loss: 1.1302 - accuracy: 0.6736 - 4s/epoch - 5ms/step\n",
            "Epoch 43/150\n",
            "782/782 - 4s - loss: 1.1376 - accuracy: 0.6708 - 4s/epoch - 5ms/step\n",
            "Epoch 44/150\n",
            "782/782 - 4s - loss: 1.1323 - accuracy: 0.6718 - 4s/epoch - 5ms/step\n",
            "Epoch 45/150\n",
            "782/782 - 4s - loss: 1.1298 - accuracy: 0.6718 - 4s/epoch - 5ms/step\n",
            "Epoch 46/150\n",
            "782/782 - 4s - loss: 1.1326 - accuracy: 0.6706 - 4s/epoch - 5ms/step\n",
            "Epoch 47/150\n",
            "782/782 - 4s - loss: 1.1300 - accuracy: 0.6719 - 4s/epoch - 5ms/step\n",
            "Epoch 48/150\n",
            "782/782 - 4s - loss: 1.1243 - accuracy: 0.6785 - 4s/epoch - 5ms/step\n",
            "Epoch 49/150\n",
            "782/782 - 4s - loss: 1.1199 - accuracy: 0.6773 - 4s/epoch - 5ms/step\n",
            "Epoch 50/150\n",
            "782/782 - 4s - loss: 1.1188 - accuracy: 0.6815 - 4s/epoch - 5ms/step\n",
            "Epoch 51/150\n",
            "782/782 - 4s - loss: 1.1172 - accuracy: 0.6818 - 4s/epoch - 5ms/step\n",
            "Epoch 52/150\n",
            "782/782 - 4s - loss: 1.1098 - accuracy: 0.6844 - 4s/epoch - 5ms/step\n",
            "Epoch 53/150\n",
            "782/782 - 4s - loss: 1.1081 - accuracy: 0.6879 - 4s/epoch - 5ms/step\n",
            "Epoch 54/150\n",
            "782/782 - 4s - loss: 1.1003 - accuracy: 0.6901 - 4s/epoch - 5ms/step\n",
            "Epoch 55/150\n",
            "782/782 - 4s - loss: 1.1039 - accuracy: 0.6904 - 4s/epoch - 5ms/step\n",
            "Epoch 56/150\n",
            "782/782 - 4s - loss: 1.1012 - accuracy: 0.6914 - 4s/epoch - 5ms/step\n",
            "Epoch 57/150\n",
            "782/782 - 4s - loss: 1.0985 - accuracy: 0.6938 - 4s/epoch - 5ms/step\n",
            "Epoch 58/150\n",
            "782/782 - 4s - loss: 1.0847 - accuracy: 0.6981 - 4s/epoch - 5ms/step\n",
            "Epoch 59/150\n",
            "782/782 - 4s - loss: 1.0887 - accuracy: 0.6971 - 4s/epoch - 5ms/step\n",
            "Epoch 60/150\n",
            "782/782 - 4s - loss: 1.0950 - accuracy: 0.6951 - 4s/epoch - 5ms/step\n",
            "Epoch 61/150\n",
            "782/782 - 4s - loss: 1.0887 - accuracy: 0.6981 - 4s/epoch - 5ms/step\n",
            "Epoch 62/150\n",
            "782/782 - 4s - loss: 1.0764 - accuracy: 0.7028 - 4s/epoch - 5ms/step\n",
            "Epoch 63/150\n",
            "782/782 - 4s - loss: 1.0851 - accuracy: 0.6999 - 4s/epoch - 5ms/step\n",
            "Epoch 64/150\n",
            "782/782 - 4s - loss: 1.0697 - accuracy: 0.7069 - 4s/epoch - 5ms/step\n",
            "Epoch 65/150\n",
            "782/782 - 4s - loss: 1.0717 - accuracy: 0.7040 - 4s/epoch - 5ms/step\n",
            "Epoch 66/150\n",
            "782/782 - 4s - loss: 1.0677 - accuracy: 0.7078 - 4s/epoch - 5ms/step\n",
            "Epoch 67/150\n",
            "782/782 - 4s - loss: 1.0732 - accuracy: 0.7053 - 4s/epoch - 5ms/step\n",
            "Epoch 68/150\n",
            "782/782 - 4s - loss: 1.0784 - accuracy: 0.7068 - 4s/epoch - 5ms/step\n",
            "Epoch 69/150\n",
            "782/782 - 4s - loss: 1.0636 - accuracy: 0.7114 - 4s/epoch - 5ms/step\n",
            "Epoch 70/150\n",
            "782/782 - 4s - loss: 1.0655 - accuracy: 0.7115 - 4s/epoch - 5ms/step\n",
            "Epoch 71/150\n",
            "782/782 - 4s - loss: 1.0612 - accuracy: 0.7140 - 4s/epoch - 5ms/step\n",
            "Epoch 72/150\n",
            "782/782 - 4s - loss: 1.0686 - accuracy: 0.7114 - 4s/epoch - 5ms/step\n",
            "Epoch 73/150\n",
            "782/782 - 4s - loss: 1.0678 - accuracy: 0.7120 - 4s/epoch - 6ms/step\n",
            "Epoch 74/150\n",
            "782/782 - 4s - loss: 1.0619 - accuracy: 0.7159 - 4s/epoch - 5ms/step\n",
            "Epoch 75/150\n",
            "782/782 - 4s - loss: 1.0545 - accuracy: 0.7186 - 4s/epoch - 5ms/step\n",
            "Epoch 76/150\n",
            "782/782 - 4s - loss: 1.0478 - accuracy: 0.7192 - 4s/epoch - 5ms/step\n",
            "Epoch 77/150\n",
            "782/782 - 4s - loss: 1.0476 - accuracy: 0.7203 - 4s/epoch - 5ms/step\n",
            "Epoch 78/150\n",
            "782/782 - 4s - loss: 1.0532 - accuracy: 0.7202 - 4s/epoch - 5ms/step\n",
            "Epoch 79/150\n",
            "782/782 - 4s - loss: 1.0481 - accuracy: 0.7211 - 4s/epoch - 5ms/step\n",
            "Epoch 80/150\n",
            "782/782 - 4s - loss: 1.0501 - accuracy: 0.7204 - 4s/epoch - 5ms/step\n",
            "Epoch 81/150\n",
            "782/782 - 4s - loss: 1.0447 - accuracy: 0.7219 - 4s/epoch - 5ms/step\n",
            "Epoch 82/150\n",
            "782/782 - 4s - loss: 1.0469 - accuracy: 0.7212 - 4s/epoch - 5ms/step\n",
            "Epoch 83/150\n",
            "782/782 - 4s - loss: 1.0447 - accuracy: 0.7229 - 4s/epoch - 5ms/step\n",
            "Epoch 84/150\n",
            "782/782 - 4s - loss: 1.0448 - accuracy: 0.7234 - 4s/epoch - 5ms/step\n",
            "Epoch 85/150\n",
            "782/782 - 4s - loss: 1.0363 - accuracy: 0.7260 - 4s/epoch - 5ms/step\n",
            "Epoch 86/150\n",
            "782/782 - 4s - loss: 1.0414 - accuracy: 0.7281 - 4s/epoch - 5ms/step\n",
            "Epoch 87/150\n",
            "782/782 - 4s - loss: 1.0372 - accuracy: 0.7279 - 4s/epoch - 5ms/step\n",
            "Epoch 88/150\n",
            "782/782 - 4s - loss: 1.0406 - accuracy: 0.7251 - 4s/epoch - 5ms/step\n",
            "Epoch 89/150\n",
            "782/782 - 4s - loss: 1.0392 - accuracy: 0.7280 - 4s/epoch - 5ms/step\n",
            "Epoch 90/150\n",
            "782/782 - 4s - loss: 1.0277 - accuracy: 0.7325 - 4s/epoch - 5ms/step\n",
            "Epoch 91/150\n",
            "782/782 - 4s - loss: 1.0346 - accuracy: 0.7321 - 4s/epoch - 5ms/step\n",
            "Epoch 92/150\n",
            "782/782 - 4s - loss: 1.0293 - accuracy: 0.7319 - 4s/epoch - 5ms/step\n",
            "Epoch 93/150\n",
            "782/782 - 4s - loss: 1.0399 - accuracy: 0.7287 - 4s/epoch - 5ms/step\n",
            "Epoch 94/150\n",
            "782/782 - 4s - loss: 1.0241 - accuracy: 0.7357 - 4s/epoch - 5ms/step\n",
            "Epoch 95/150\n",
            "782/782 - 4s - loss: 1.0320 - accuracy: 0.7307 - 4s/epoch - 5ms/step\n",
            "Epoch 96/150\n",
            "782/782 - 4s - loss: 1.0329 - accuracy: 0.7349 - 4s/epoch - 5ms/step\n",
            "Epoch 97/150\n",
            "782/782 - 4s - loss: 1.0333 - accuracy: 0.7343 - 4s/epoch - 5ms/step\n",
            "Epoch 98/150\n",
            "782/782 - 4s - loss: 1.0262 - accuracy: 0.7357 - 4s/epoch - 5ms/step\n",
            "Epoch 99/150\n",
            "782/782 - 4s - loss: 1.0277 - accuracy: 0.7364 - 4s/epoch - 5ms/step\n",
            "Epoch 100/150\n",
            "782/782 - 4s - loss: 1.0218 - accuracy: 0.7387 - 4s/epoch - 5ms/step\n",
            "Epoch 101/150\n",
            "782/782 - 4s - loss: 1.0297 - accuracy: 0.7370 - 4s/epoch - 5ms/step\n",
            "Epoch 102/150\n",
            "782/782 - 4s - loss: 1.0258 - accuracy: 0.7355 - 4s/epoch - 5ms/step\n",
            "Epoch 103/150\n",
            "782/782 - 4s - loss: 1.0253 - accuracy: 0.7386 - 4s/epoch - 5ms/step\n",
            "Epoch 104/150\n",
            "782/782 - 4s - loss: 1.0210 - accuracy: 0.7371 - 4s/epoch - 5ms/step\n",
            "Epoch 105/150\n",
            "782/782 - 4s - loss: 1.0222 - accuracy: 0.7385 - 4s/epoch - 5ms/step\n",
            "Epoch 106/150\n",
            "782/782 - 4s - loss: 1.0285 - accuracy: 0.7350 - 4s/epoch - 5ms/step\n",
            "Epoch 107/150\n",
            "782/782 - 4s - loss: 1.0211 - accuracy: 0.7379 - 4s/epoch - 5ms/step\n",
            "Epoch 108/150\n",
            "782/782 - 4s - loss: 1.0209 - accuracy: 0.7380 - 4s/epoch - 5ms/step\n",
            "Epoch 109/150\n",
            "782/782 - 4s - loss: 1.0226 - accuracy: 0.7404 - 4s/epoch - 5ms/step\n",
            "Epoch 110/150\n",
            "782/782 - 4s - loss: 1.0219 - accuracy: 0.7393 - 4s/epoch - 5ms/step\n",
            "Epoch 111/150\n",
            "782/782 - 4s - loss: 1.0178 - accuracy: 0.7412 - 4s/epoch - 5ms/step\n",
            "Epoch 112/150\n",
            "782/782 - 4s - loss: 1.0151 - accuracy: 0.7430 - 4s/epoch - 5ms/step\n",
            "Epoch 113/150\n",
            "782/782 - 4s - loss: 1.0133 - accuracy: 0.7449 - 4s/epoch - 5ms/step\n",
            "Epoch 114/150\n",
            "782/782 - 4s - loss: 1.0207 - accuracy: 0.7401 - 4s/epoch - 5ms/step\n",
            "Epoch 115/150\n",
            "782/782 - 4s - loss: 1.0137 - accuracy: 0.7433 - 4s/epoch - 5ms/step\n",
            "Epoch 116/150\n",
            "782/782 - 4s - loss: 1.0178 - accuracy: 0.7418 - 4s/epoch - 5ms/step\n",
            "Epoch 117/150\n",
            "782/782 - 4s - loss: 1.0091 - accuracy: 0.7450 - 4s/epoch - 5ms/step\n",
            "Epoch 118/150\n",
            "782/782 - 4s - loss: 1.0204 - accuracy: 0.7425 - 4s/epoch - 5ms/step\n",
            "Epoch 119/150\n",
            "782/782 - 4s - loss: 1.0161 - accuracy: 0.7434 - 4s/epoch - 5ms/step\n",
            "Epoch 120/150\n",
            "782/782 - 4s - loss: 1.0118 - accuracy: 0.7449 - 4s/epoch - 5ms/step\n",
            "Epoch 121/150\n",
            "782/782 - 4s - loss: 1.0117 - accuracy: 0.7468 - 4s/epoch - 5ms/step\n",
            "Epoch 122/150\n",
            "782/782 - 4s - loss: 1.0128 - accuracy: 0.7458 - 4s/epoch - 5ms/step\n",
            "Epoch 123/150\n",
            "782/782 - 4s - loss: 1.0129 - accuracy: 0.7475 - 4s/epoch - 5ms/step\n",
            "Epoch 124/150\n",
            "782/782 - 4s - loss: 1.0070 - accuracy: 0.7473 - 4s/epoch - 5ms/step\n",
            "Epoch 125/150\n",
            "782/782 - 4s - loss: 1.0150 - accuracy: 0.7440 - 4s/epoch - 5ms/step\n",
            "Epoch 126/150\n",
            "782/782 - 4s - loss: 1.0017 - accuracy: 0.7490 - 4s/epoch - 5ms/step\n",
            "Epoch 127/150\n",
            "782/782 - 4s - loss: 1.0107 - accuracy: 0.7482 - 4s/epoch - 5ms/step\n",
            "Epoch 128/150\n",
            "782/782 - 4s - loss: 1.0129 - accuracy: 0.7497 - 4s/epoch - 5ms/step\n",
            "Epoch 129/150\n",
            "782/782 - 4s - loss: 1.0084 - accuracy: 0.7484 - 4s/epoch - 6ms/step\n",
            "Epoch 130/150\n",
            "782/782 - 4s - loss: 1.0043 - accuracy: 0.7502 - 4s/epoch - 5ms/step\n",
            "Epoch 131/150\n",
            "782/782 - 4s - loss: 1.0042 - accuracy: 0.7493 - 4s/epoch - 5ms/step\n",
            "Epoch 132/150\n",
            "782/782 - 4s - loss: 1.0035 - accuracy: 0.7470 - 4s/epoch - 5ms/step\n",
            "Epoch 133/150\n",
            "782/782 - 4s - loss: 1.0098 - accuracy: 0.7477 - 4s/epoch - 5ms/step\n",
            "Epoch 134/150\n",
            "782/782 - 4s - loss: 1.0022 - accuracy: 0.7510 - 4s/epoch - 5ms/step\n",
            "Epoch 135/150\n",
            "782/782 - 4s - loss: 1.0073 - accuracy: 0.7499 - 4s/epoch - 5ms/step\n",
            "Epoch 136/150\n",
            "782/782 - 4s - loss: 1.0116 - accuracy: 0.7503 - 4s/epoch - 5ms/step\n",
            "Epoch 137/150\n",
            "782/782 - 4s - loss: 0.9955 - accuracy: 0.7548 - 4s/epoch - 5ms/step\n",
            "Epoch 138/150\n",
            "782/782 - 4s - loss: 1.0088 - accuracy: 0.7488 - 4s/epoch - 5ms/step\n",
            "Epoch 139/150\n",
            "782/782 - 4s - loss: 1.0087 - accuracy: 0.7489 - 4s/epoch - 5ms/step\n",
            "Epoch 140/150\n",
            "782/782 - 4s - loss: 1.0018 - accuracy: 0.7540 - 4s/epoch - 5ms/step\n",
            "Epoch 141/150\n",
            "782/782 - 4s - loss: 1.0033 - accuracy: 0.7511 - 4s/epoch - 5ms/step\n",
            "Epoch 142/150\n",
            "782/782 - 4s - loss: 0.9968 - accuracy: 0.7546 - 4s/epoch - 5ms/step\n",
            "Epoch 143/150\n",
            "782/782 - 4s - loss: 1.0008 - accuracy: 0.7518 - 4s/epoch - 5ms/step\n",
            "Epoch 144/150\n",
            "782/782 - 4s - loss: 0.9979 - accuracy: 0.7542 - 4s/epoch - 5ms/step\n",
            "Epoch 145/150\n",
            "782/782 - 4s - loss: 1.0050 - accuracy: 0.7540 - 4s/epoch - 5ms/step\n",
            "Epoch 146/150\n",
            "782/782 - 4s - loss: 0.9958 - accuracy: 0.7567 - 4s/epoch - 5ms/step\n",
            "Epoch 147/150\n",
            "782/782 - 4s - loss: 0.9917 - accuracy: 0.7558 - 4s/epoch - 5ms/step\n",
            "Epoch 148/150\n",
            "782/782 - 4s - loss: 0.9946 - accuracy: 0.7550 - 4s/epoch - 5ms/step\n",
            "Epoch 149/150\n",
            "782/782 - 4s - loss: 0.9961 - accuracy: 0.7531 - 4s/epoch - 5ms/step\n",
            "Epoch 150/150\n",
            "782/782 - 4s - loss: 1.0034 - accuracy: 0.7537 - 4s/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e749c3eb160>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = model.evaluate(x_test, y_test, batch_size=64, verbose=2)\n",
        "print(f'{acc:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qiP_mwSbxHe",
        "outputId": "03517727-fd7d-4084-fe17-9a9dd3414db6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "157/157 - 1s - loss: 1.1784 - accuracy: 0.7331 - 648ms/epoch - 4ms/step\n",
            "0.73\n"
          ]
        }
      ]
    }
  ]
}